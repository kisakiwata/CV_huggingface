# -*- coding: utf-8 -*-
"""PDF_Reader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hi99HCYaVIlBCmczdfDiPZRefEfu6R5n
"""

!pip install --quiet --upgrade langchain
!pip install --quiet pypdf
!pip install --quiet load_dotenv
!pip install --quiet cohere tiktoken openai
! pip install --quiet faiss-cpu
! pip install --quiet unstructured
! pip install --quiet pdf2image
! pip install --quiet PyPDF2
! pip install chromadb
! pip install pdfminer

from google.colab import drive
drive.mount('/content/drive')

# check if the path exists
import dotenv
import os
os.path.isdir('/content/drive/MyDrive/Colab/Transformer')

dotenv.load_dotenv(os.path.join('/content/drive/MyDrive/Colab/Transformer', '.env'))

import langchain
import openai
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import OnlinePDFLoader
import tiktoken
import requests
from langchain.storage import LocalFileStore
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import TokenTextSplitter
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
import glob
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.indexes import VectorstoreIndexCreator
from requests.exceptions import ReadTimeout, ConnectionError, ConnectTimeout
from PyPDF2 import PdfReader
from langchain.docstore.document import Document
from langchain.document_loaders.pdf import PDFPlumberLoader, PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.vectorstores import Chroma

"""# make documents one single PDF first and then load to textloader + splitdocumetns"""

def find_latest_txt_file(directory):
    # Get a list of text files in the specified directory
    txt_files = glob.glob(os.path.join(directory, '*.txt'))

    if not txt_files:
        return None  # No text files found in the directory

    # Sort the text files by modification time (newest first)
    txt_files.sort(key=os.path.getmtime, reverse=True)

    # Return the path of the latest text file
    return txt_files[0]


# where our embeddings will be stored
store = LocalFileStore("./cache/")

# Path to the text file containing PDF links
dirname = '/content/drive/MyDrive/Colab/Transformer'
LINK_DIRECTORY = os.path.join(dirname,'link_collection')
text_file_path = find_latest_txt_file(LINK_DIRECTORY)

# Read the text file and load the PDFs
with open(text_file_path, 'r') as file:
    pdf_links = file.read().splitlines()

# store all the pdfs under data folder

import os
import requests

# Create a "data" folder if it doesn't exist
if not os.path.exists("data"):
    os.makedirs("data")

for pdf_link in pdf_links:
    try:
        response = requests.get(pdf_link, timeout=10)  # Adjust the timeout value as needed
        if response.status_code == 200:
            # Extract the filename from the URL
            filename = pdf_link.split("/")[-1]
            # Combine with "data/" to specify the folder
            file_path = os.path.join("data", filename)

            with open(file_path, 'wb') as f:
                f.write(response.content)
                print(f"PDF link {pdf_link} downloaded and saved as {file_path}")
        else:
            print(f"PDF link {pdf_link} returned status code {response.status_code}. Skipping...")
    except Exception as e:
        print(f"Error processing {pdf_link}: {e}")

from PyPDF2 import PdfMerger
import requests
import sys
import os

# Create a "data" folder if it doesn't exist
if not os.path.exists("data"):
    os.makedirs("data")

merger = PdfMerger()
for pdf_link in pdf_links:
    try:
        response = requests.get(pdf_link, timeout=10)  # Adjust the timeout value as needed
        if response.status_code == 200:
            # Extract the filename from the URL
            filename = pdf_link.split("/")[-1]
            # Combine with "data/" to specify the folder
            file_path = os.path.join("data", filename)
            with open(file_path, 'wb') as f:
                f.write(response.content)
                print(f"PDF link {pdf_link} downloaded and saved as {file_path}")

        else:
            print(f"PDF link {pdf_link} returned status code {response.status_code}. Skipping...")
    # except PdfReadError as e:
    #         print(title)
    #         sys.exit()

    except Exception as e:
        print(f"Error processing {pdf_link}: {e}")


merger.append(filename)
# Save the merged PDF as "result.pdf"
merger.write("result.pdf")
merger.close()



import os
from PyPDF2 import PdfMerger

# List all PDF files in the "data" folder
pdfs = [os.path.join("data", filename) for filename in os.listdir("data") if filename.endswith(".pdf")]

merger = PdfMerger()

for pdf in pdfs:
    try:
        merger.append(pdf)
    except Exception as e:
        print(f"Error appending {pdf}: {e}")

# Save the merged PDF as "result.pdf"
merger.write("result.pdf")
merger.close()

from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import resolve1

# check the length of pages


file = open("/content/result.pdf", 'rb')
parser = PDFParser(file)
document = PDFDocument(parser)

# This will give you the count of pages
print(resolve1(document.catalog['Pages'])['Count'])

# Create a TokenTextSplitter instance
text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)

pdf_loader = PyPDFLoader("/content/result.pdf") #UnstructuredPDFLoader not working

pages = pdf_loader.load_and_split()


# Split the single document into chunks using the TokenTextSplitter
# pages = text_splitter.split_text(single_document)
print(f"length of pages of single document is {len(pages)}")

# instantiate embedding model
embeddings_model = OpenAIEmbeddings()

embedder = CacheBackedEmbeddings.from_bytes_store(
    embeddings_model,
    store
)

# create vector store, we use FAISS in this case
vector_store = FAISS.from_documents(pages, embedder)

# # you could use FAISS or Chroma for vector storages
# db = Chroma.from_documents(pages, embedder)
# retriever = db.as_retriever()

# this is the entire retrieval system
medium_qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=vector_store.as_retriever(), #retriever, #,
    return_source_documents=True,
    verbose=True
)


sample_prompts = ["What is the most common application of machine learning for inventory maangement?",
                  "What is the most common application of machine learning for demand forcasting in supply chain maangement?"]

for prompt in sample_prompts:

    #vanilla OpenAI Response
    response = openmixer.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens = 500)

    # RAG Augmented Response
    response_rag = medium_qa_chain({"query":prompt})

"""# starts from here to run extracted PDF"""

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Create 2 virtual GPUs with 1GB memory each
  try:
    tf.config.set_logical_device_configuration(
        gpus[0],
        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),
         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPU,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Virtual devices must be set before GPUs have been initialized
    print(e)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

# import locale
# print(locale.getpreferredencoding())

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install transformers accelerate einops langchain xformers bitsandbytes chromadb sentence_transformers

!pip install openai==0.28

# !pip install light-the-torch
# !ltt install torch torchvision

! pip install pypdf

from google.colab import drive
drive.mount('/content/drive')

from torch import cuda, bfloat16
import torch
import transformers
from transformers import AutoTokenizer, GPTQConfig
from time import time
import chromadb
from chromadb.config import Settings
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.document_loaders.pdf import PDFPlumberLoader, PyPDFLoader

!huggingface-cli login

model_id = 'meta-llama/Llama-2-7b-hf'

#device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

# set quantization configuration to load large model with less GPU memory
# this requires the `bitsandbytes` library
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)

"""# separate into different batches

# per_device_train_batch_size=4,

#trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=transformers.TrainingArguments(

https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo




"""

!nvidia-smi

# import torch
# torch.cuda.is_available()

# ! pip install flash-attn --no-build-isolation

import torch

# Check if a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# Your model and data operations
# model.to(device)

time_1 = time()
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization = GPTQConfig(bits=4,dataset = "c4", tokenizer=tokenizer)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    #quantization_config=quantization,
    device_map='auto',
    #load_in_8bit_fp32_cpu_offload=True,
    #load_in_8bit=True
)
# put the model into cuda

#model.to(device)

time_2 = time()
print(f"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.")

time_1 = time()
query_pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto",
        batch_size=8)
time_2 = time()
print(f"Prepare pipeline: {round(time_2-time_1, 3)} sec.")

#!pip install openai==0.28

#testing OPEN AI

# sample_prompts = ["What is the most common application of machine learning for inventory maangement?",
#                   "What is the most common application of machine learning for demand forcasting in supply chain maangement?"]

# for prompt in sample_prompts:

#     #vanilla OpenAI Response
#     response = openai.Completion.create(
#         engine="text-davinci-003",
#         prompt=prompt,
#         max_tokens = 500)

#     # RAG Augmented Response
#     response_rag = medium_qa_chain({"query":prompt})

def test_model(tokenizer, pipeline, prompt_to_test):
    """
    Perform a query
    print the result
    Args:
        tokenizer: the tokenizer
        pipeline: the pipeline
        prompt_to_test: the prompt
    Returns
        None
    """
    # adapted from https://huggingface.co/blog/llama2#using-transformers
    time_1 = time()
    sequences = pipeline(
        prompt_to_test,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=200,)
    time_2 = time()
    print(f"Test inference: {round(time_2-time_1, 3)} sec.")
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")

# skip this as it takes too much time
test_model(tokenizer,
           query_pipeline,
           "What is the most common application of machine learning for inventory management?")

# from transformers import pipeline
# llm = pipeline(pipeline=query_pipeline)
# # checking again that everything is working fine
# llm.tokenizer.pad_token_id = llm.config.eos_token_id,
# llm(prompt="What is the most common application of machine learning for inventory management?", )

# Create a TokenTextSplitter instance
#text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)

pdf_file_path = '/content/drive/My Drive/Colab/Transformer/result.pdf'

pdf_loader = PyPDFLoader(pdf_file_path) #UnstructuredPDFLoader not working

pages = pdf_loader.load_and_split()




# loader = TextLoader("/content/result.pdf",
#                     encoding="utf8")
# documents = loader.load()

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
# all_splits = text_splitter.split_documents(documents)

model_name = "bert-base-uncased" #sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "cuda"}

# from transformers import BertConfig, BertModel

# # Initializing a BERT bert-base-uncased style configuration
# configuration = BertConfig()

# # Initializing a model (with random weights) from the bert-base-uncased style configuration
# model = BertModel(configuration)

#embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

# db = Chroma.from_documents(pages, embeddings)
# retriever = db.as_retriever()

# import torch

# # Check if a GPU is available
# if torch.cuda.is_available():
#     device = torch.device("cuda")
# else:
#     device = torch.device("cpu")

# # Your model and data operations
# # model.to(device)
#

pages = pages.to(device)

vectordb = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory="chroma_db")

retriever = vectordb.as_retriever()
llm = HuggingFacePipeline(pipeline=query_pipeline)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    verbose=True
)

def test_rag(qa, query):
    print(f"Query: {query}\n")
    time_1 = time()
    result = qa.run(query)
    time_2 = time()
    print(f"Inference time: {round(time_2-time_1, 3)} sec.")
    print("\nResult: ", result)

import torch
torch.cuda.empty_cache()

tf.debugging.set_log_device_placement(True)
gpus = tf.config.list_logical_devices('GPU')
strategy = tf.distribute.MirroredStrategy(gpus)
with strategy.scope():
  inputs = tf.keras.layers.Input(shape=(1,))
  predictions = tf.keras.layers.Dense(1)(inputs)
  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
  model.compile(loss='mse',
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))

query = "What is the most common application of machine learning for inventory management? Summarize. Keep it under 200 words."
test_rag(qa, query)

query = "WWhat is the most common application of machine learning for demand forcasting in supply chain management? Summarize. Keep it under 200 words."
test_rag(qa, query)

docs = vectordb.similarity_search(query)
print(f"Query: {query}")
print(f"Retrieved documents: {len(docs)}")
for doc in docs:
    doc_details = doc.to_json()['kwargs']
    print("Source: ", doc_details['metadata']['source'])
    print("Text: ", doc_details['page_content'], "\n")